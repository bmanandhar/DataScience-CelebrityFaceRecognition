{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745d690d",
   "metadata": {},
   "source": [
    "Frist of all, we import the modules that are required for preprocessing data, images in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, warnings, matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore', '.*do not*')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150a5dc",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "\n",
    "It is assumed that a person is recognized or identified by reading or observing his/her face. When we consider the face to recognize, it is obvious that we basically read the two eyes on the face.\n",
    "\n",
    "In this project, Python modules are used to detect face, then eyes. If the image does not have a face that can be viewed clearly or if the face is obstructed or the two eyes are not clearly visible, we ignore or discard that image.\n",
    "\n",
    "Here is an experiment where we intake example image and convert it into numpy array as preparation of data for a machine learning classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_images/federer1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8907e",
   "metadata": {},
   "source": [
    "cv2 nodule by default creates numpy array for a color image based on BGR(Blue-Green-Red) spaces. Therefore, it's a three dimensional expression. If it's a gray image, array will be two dimensional. Data here is three dimesional. x, y coordinates and rgb representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe02ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8023b6e",
   "metadata": {},
   "source": [
    "Since the image has more of the unwanted area, we crop it out so as to keep only the required portion by using numpy array properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cbc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop image to get required area only\n",
    "img = img[0:600, 850:1400]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c8c03",
   "metadata": {},
   "source": [
    "Let's have the RGB version of the same image by using the module that we imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08c371",
   "metadata": {},
   "source": [
    "As a matter of fact, image can also be converted to RGB spaces by using adjusting the numpy array like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgr converted to rgb\n",
    "plt.imshow(img[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1630c8",
   "metadata": {},
   "source": [
    "Let's also have the gray image data, it is two dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a11de",
   "metadata": {},
   "source": [
    "Detet faces and eyes using openCV library and haarcascade thechnique. It provides us a rectangular location where faces and eyes lie by generating list of lists, if it sees multiple faces. For each face, there is a list of four elements representing x / y coordinates, width and height of the image. Then, the technique to detect eyes will work on each face.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect x,y,w,h \n",
    "face_cascade = cv2.CascadeClassifier('./opencv/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('./opencv/haarcascades/haarcascade_eye.xml')\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0e53c",
   "metadata": {},
   "source": [
    "By definition, the four elements in above np array represent x-pos, y-pos, height and width of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y,w,h) = faces[0]\n",
    "f'{x= }, {y= }, {w= }, {h= }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eba579",
   "metadata": {},
   "source": [
    "We now draw a rectangle covering only the face by using cv2 module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013de6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_img = cv2.rectangle(img, (x,y),(x+w,y+h),(255,0,0),2)\n",
    "plt.imshow(face_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48233c10",
   "metadata": {},
   "source": [
    "After detecting face, we then detect two eyes present on the face image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "for (x,y,w,h) in faces:\n",
    "    face_img = cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 1)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = face_img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex, ey, ew, eh) in eyes:\n",
    "        cv2.rectangle(roi_color, (ex, ey), (ex+ew, ey+eh), (0,255,0), 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(face_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4549eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(roi_color, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30864670",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img = np.array(roi_color)\n",
    "cropped_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677fbf5",
   "metadata": {},
   "source": [
    "The cropped image is now converted into wavelets by using wavelt transform module. This technique is used because wavelet transform has several characterstics: it can provide horizontal, vertical, and diagonal multi-directional information. The information is distributed in each individual pixel without interfering with each other, and it can achieve lossless reconstruction in process of image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab87b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wavelet transform\n",
    "import pywt, cv2\n",
    "\n",
    "def w2d(img, mode='haar', level=1):\n",
    "    imArray = img\n",
    "    #Datatype conversions\n",
    "    #convert to grayscale, step-1\n",
    "    imArray = cv2.cvtColor(imArray,cv2.COLOR_RGB2GRAY)\n",
    "    #convert to float, step-2\n",
    "    imArray =  np.float32(imArray) \n",
    "    imArray /= 255;\n",
    "    # compute coefficients \n",
    "    coeffs = pywt.wavedec2(imArray, mode, level=level)\n",
    "\n",
    "    #Process Coefficients\n",
    "    coeffs_H = list(coeffs)  \n",
    "    coeffs_H[0] *= 0;  \n",
    "\n",
    "    # reconstruction\n",
    "    imArray_H = pywt.waverec2(coeffs_H, mode);\n",
    "    imArray_H *= 255;\n",
    "    imArray_H =  np.uint8(imArray_H)\n",
    "\n",
    "    return imArray_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_har = w2d(cropped_img, 'db1', 5)\n",
    "plt.imshow(im_har, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ee090",
   "metadata": {},
   "source": [
    "Function to obtain cropped images if two eyes are clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_image_if_2_eyes(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        if len(eyes) >= 2:\n",
    "            return roi_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = cv2.imread('./test_images/federer1.jpg')[0:600, 850:1400]\n",
    "plt.imshow(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010623f",
   "metadata": {},
   "source": [
    "unuseful image, returns None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289a0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_image_obstructed = cv2.imread('./test_images/federer2.jpg')\n",
    "plt.imshow(orig_image_obstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aee63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_image_no_2_eyes = get_cropped_image_if_2_eyes('./test_images/messi2.jpg')\n",
    "cropped_image_no_2_eyes #returns None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1911b70",
   "metadata": {},
   "source": [
    "#### Managing variables to save cropped images of useful original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ab8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = './dataset/'\n",
    "path_to_cr_data = ''.join([path_to_data, 'cropped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5437c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "img_dirs = []\n",
    "\n",
    "for entry in os.scandir(path_to_data):\n",
    "    if not entry.name.startswith('.') and entry.name != 'cropped':\n",
    "        img_dirs.append(entry.path)\n",
    "        \n",
    "img_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d220792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists(path_to_cr_data):\n",
    "    shutil.rmtree(path_to_cr_data)\n",
    "os.mkdir(path_to_cr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96572c45",
   "metadata": {},
   "source": [
    "Collection of cropped images as preparation for model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5774ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_image_dirs = []\n",
    "celebrity_file_names_dict = {}\n",
    "\n",
    "for img_dir in img_dirs:\n",
    "    count = 1\n",
    "    celebrity_name = img_dir.split('/')[-1]\n",
    "    celebrity_file_names_dict[celebrity_name] = []\n",
    "    \n",
    "    for entry in os.scandir(img_dir):\n",
    "        roi_color = get_cropped_image_if_2_eyes(entry.path)\n",
    "        \n",
    "        if roi_colo is not None:\n",
    "            cropped_folder = '/'.join([path_to_cr_data, celebrity_name])\n",
    "            \n",
    "            if not os.path.exists(cropped_folder):\n",
    "                os.makedirs(cropped_folder)\n",
    "                cropped_image_dirs.append(cropped_folder)\n",
    "                print(f'Generating cropped images in folder: {cropped_folder}')\n",
    "                \n",
    "            cropped_file_name = ''.join([celebrity_name, f'{count}', '.png'])\n",
    "            cropped_file_path = '/'.join([cropped_folder, cropped_file_name])\n",
    "            cv2.imwrite(cropped_file_path, roi_color)\n",
    "            celebrity_file_names_dict[celebrity_name].append(cropped_file_path)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ec273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# celebrity_file_name_dict = {}\n",
    "class_dict = {}\n",
    "\n",
    "count = 0\n",
    "for img_dir in cropped_image_dirs:\n",
    "    celebrity_name = img_dir.split('/')[-1]\n",
    "    class_dict[celebrity_name] = count\n",
    "    \n",
    "    file_list = []\n",
    "    for entry in os.scandir(img_dir):\n",
    "        file_list.append(entry.path)\n",
    "        \n",
    "    celebrity_file_names_dict[celebrity_name] = file_list\n",
    "    count += 1\n",
    "    \n",
    "print(celebrity_file_names_dict['serena_williams'][:3])\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stackoverflow\n",
    "X, y = [], []\n",
    "for celebrity_name, training_files in celebrity_file_names_dict.items():\n",
    "    for training_image in training_files:\n",
    "        img = cv2.imread(training_image)\n",
    "        scalled_raw_img = cv2.resize(img, (32, 32))\n",
    "        img_har = w2d(img, 'db1', 5)\n",
    "        scalled_img_har = cv2.resize(img_har, (32, 32))\n",
    "        combined_img = np.vstack([scalled_raw_img.reshape(32*32*3, 1), scalled_img_har.reshape(32*32, 1)])\n",
    "        X.append(combined_img)\n",
    "        y.append(class_dict[celebrity_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f718de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(len(X), 4096).astype(float)\n",
    "X.shape\n",
    "#data cleaning process is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiative svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a669552",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', C=10))])\n",
    "pipe.fit(X_train, y_train)\n",
    "np.round(pipe.score(X_test, y_test), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "#1\n",
    "    'svm': {\n",
    "        'model': svm.SVC(gamma='auto',probability=True),\n",
    "        'params': {\n",
    "            'svc__C': [1,10,100,1000],\n",
    "            'svc__kernel': ['rbf','linear']            \n",
    "        }\n",
    "    },\n",
    "#2  \n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'randomforestclassifier__n_estimators': [1,5,10]\n",
    "        }\n",
    "    },\n",
    "#3  \n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'logisticregression__C': [1,5,10]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "best_estimators = {}\n",
    "import pandas as pd\n",
    "\n",
    "for algo, mp in model_params.items():\n",
    "    pipe = make_pipeline(StandardScaler(), mp['model'])\n",
    "    clf = GridSearchCV(pipe, mp['params'], cv=5, return_train_score=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append({\n",
    "        'model': algo,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_\n",
    "    })\n",
    "    best_estimators[algo] = clf.best_estimator_\n",
    "    \n",
    "df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators['svm'].score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators['random_forest'].score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators['logistic_regression'].score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = best_estimators['logistic_regression']\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, best_clf.predict(X_test))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib\n",
    "import joblib \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(best_clf, 'saved_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"class_dictionary.json\",\"w\") as f:\n",
    "    f.write(json.dumps(class_dict))\n",
    "'Code Finished...'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
